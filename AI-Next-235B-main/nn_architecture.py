"""
Implementation of the world's best neural network based on the described architecture.
This network consists of:
- 2 initial layers: Embedding and Positional Encoding
- 12 identical blocks, each containing 8 layers (96 total block layers)
- 3 final layers: Final RMSNorm, Output Layer (Unembedding), and Softmax/Sampling
Total: ~99-101 layers with approximately 235 billion parameters
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, List


class RMSNorm(nn.Module):
    """
    Root Mean Square Layer Normalization
    """
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Calculate RMS
        rms = torch.sqrt(torch.mean(x.pow(2), dim=-1, keepdim=True) + self.eps)
        # Normalize and scale
        return self.weight * (x / rms)


class GatedDeltaNet(nn.Module):
    """
    Gated Linear Attention mechanism inspired by recent efficient attention mechanisms
    This implements a linear attention variant that processes sequences efficiently
    """
    def __init__(self, dim: int, head_dim: int = 64, num_heads: int = 16):
        super().__init__()
        self.dim = dim
        self.head_dim = head_dim
        self.num_heads = num_heads
        self.scale = head_dim ** -0.5
        
        # Input projections
        self.to_qkv = nn.Linear(dim, 3 * head_dim * num_heads, bias=False)
        self.to_gate = nn.Linear(dim, head_dim * num_heads, bias=False)
        
        # Output projection
        self.output_projection = nn.Linear(head_dim * num_heads, dim, bias=False)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, T, C = x.shape
        
        # Project to Q, K, V and gate
        qkv = self.to_qkv(x).reshape(B, T, 3, self.num_heads, self.head_dim)
        q, k, v = qkv.unbind(dim=2)  # Each is [B, T, H, D]
        
        # Compute gate
        gate_input = self.to_gate(x).reshape(B, T, self.num_heads, self.head_dim)
        
        # Apply linear attention mechanism
        # Using exponential moving average for efficiency
        q = F.elu(q) + 1.0
        k = F.elu(k) + 1.0
        
        # Compute attention in linear time
        k_cumsum = k.cumsum(dim=1)  # [B, T, H, D]
        kv = (k.unsqueeze(-3) * v.unsqueeze(-4)).cumsum(dim=1)  # [B, T, H, D, D]
        
        # Compute numerator and denominator
        numerator = (q.unsqueeze(-3) * kv).sum(dim=-1)  # [B, T, H, D]
        denominator = (q.unsqueeze(-3) * k_cumsum).sum(dim=-1)  # [B, T, H, D]
        
        # Compute output
        output = numerator / (denominator + 1e-6)
        
        # Apply gating mechanism
        gate = torch.sigmoid(gate_input)
        output = output * gate
        
        # Reshape and project back
        output = output.reshape(B, T, self.num_heads * self.head_dim)
        return self.output_projection(output)


class MixtureOfExperts(nn.Module):
    """
    High-capacity Mixture of Experts layer
    Uses top-k routing to select experts for computation
    Supports 256 experts with 42 active per token (except 2 special tokens)
    """
    def __init__(self, dim: int, num_experts: int = 256, expert_size: int = 2048, active_experts: int = 42):
        super().__init__()
        self.num_experts = num_experts
        self.expert_size = expert_size
        self.active_experts = active_experts  # Number of active experts per token
        
        # Expert networks - 256 experts as per requirements
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(dim, expert_size),
                nn.GELU(),
                nn.Linear(expert_size, dim)
            ) for _ in range(num_experts)
        ])
        
        # Router network to compute expert weights
        self.router = nn.Linear(dim, num_experts)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, T, C = x.shape
        
        # Compute routing weights
        router_logits = self.router(x)  # [B, T, num_experts]
        router_weights = F.softmax(router_logits, dim=-1)  # [B, T, num_experts]
        
        # Get top-k experts for each token (42 active experts as per requirements)
        top_weights, top_indices = torch.topk(router_weights, self.active_experts, dim=-1)  # [B, T, active_experts]
        
        # Normalize top-k weights
        top_weights = F.softmax(top_weights, dim=-1)
        
        # Process tokens through selected experts
        final_output = torch.zeros_like(x)
        
        # Vectorized computation for efficiency
        for b in range(B):
            for t in range(T):
                # Get the active experts for this token
                active_expert_weights = top_weights[b, t, :]  # [active_experts]
                active_expert_indices = top_indices[b, t, :]  # [active_experts]
                
                # Get token input
                token_input = x[b:b+1, t:t+1, :]  # [1, 1, C]
                
                # Process through all active experts and aggregate
                token_output = torch.zeros_like(token_input.squeeze(0).squeeze(0))  # [C]
                for k in range(self.active_experts):
                    expert_id = active_expert_indices[k].item()
                    expert_weight = active_expert_weights[k]
                    
                    expert_output = self.experts[expert_id](token_input)  # [1, 1, C]
                    token_output += expert_weight * expert_output.squeeze(0).squeeze(0)  # [C]
                
                final_output[b, t, :] = token_output
        
        return final_output


class GroupedQueryAttention(nn.Module):
    """
    Grouped Query Attention - more efficient than Multi-head Attention
    """
    def __init__(self, dim: int, num_heads: int = 16, group_size: int = 4):
        super().__init__()
        assert dim % num_heads == 0, "dim must be divisible by num_heads"
        
        self.num_heads = num_heads
        self.group_size = group_size
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5
        
        # Number of key-value groups
        self.num_groups = max(1, num_heads // group_size)
        
        # Projections
        self.to_q = nn.Linear(dim, num_heads * self.head_dim, bias=False)
        self.to_k = nn.Linear(dim, self.num_groups * self.head_dim, bias=False)
        self.to_v = nn.Linear(dim, self.num_groups * self.head_dim, bias=False)
        self.output_projection = nn.Linear(num_heads * self.head_dim, dim, bias=False)
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        B, T, C = x.shape
        
        # Project to Q, K, V
        q = self.to_q(x).reshape(B, T, self.num_heads, self.head_dim)
        k = self.to_k(x).reshape(B, T, self.num_groups, self.head_dim)
        v = self.to_v(x).reshape(B, T, self.num_groups, self.head_dim)
        
        # Expand K and V to match number of heads
        k = k.repeat_interleave(self.num_heads // self.num_groups, dim=2)
        v = v.repeat_interleave(self.num_heads // self.num_groups, dim=2)
        
        # Transpose for attention computation
        q = q.transpose(1, 2)  # [B, H, T, D]
        k = k.transpose(1, 2)  # [B, H, T, D]
        v = v.transpose(1, 2)  # [B, H, T, D]
        
        # Compute attention scores
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # [B, H, T, T]
        
        # Apply mask if provided
        if mask is not None:
            attn_scores.masked_fill_(mask == 0, float('-inf'))
        
        # Apply softmax
        attn_weights = F.softmax(attn_scores, dim=-1)
        
        # Apply attention to values
        output = torch.matmul(attn_weights, v)  # [B, H, T, D]
        
        # Reshape and project back
        output = output.transpose(1, 2).contiguous().reshape(B, T, -1)
        return self.output_projection(output)


class MultiTokenPredictionHead(nn.Module):
    """
    Specialized head that allows predicting multiple tokens simultaneously
    """
    def __init__(self, dim: int, prediction_horizon: int = 3, vocab_size: int = 50432):
        super().__init__()
        self.prediction_horizon = prediction_horizon
        self.vocab_size = vocab_size
        
        # Predict multiple future tokens
        self.token_predictors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(dim, dim),
                nn.GELU(),
                nn.Linear(dim, vocab_size)
            ) for _ in range(prediction_horizon)
        ])
        
    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:
        predictions = []
        for predictor in self.token_predictors:
            pred = predictor(x)
            predictions.append(pred)
        return predictions


class SuperBlock(nn.Module):
    """
    A single superblock containing 8 specialized layers as per your architecture
    """
    def __init__(self, dim: int, num_heads: int = 16, head_dim: int = 64, 
                 num_experts: int = 256, expert_size: int = 2048, active_experts: int = 42, vocab_size: int = 50432):
        super().__init__()
        
        # Layer 1: RMSNorm
        self.norm1 = RMSNorm(dim)
        
        # Layer 2: Gated DeltaNet (Linear Attention)
        self.deltanet = GatedDeltaNet(dim, head_dim, num_heads)
        
        # Layer 3: MoE FFN Layer (High-Capacity) - 256 experts, 42 active
        self.moe_ffn1 = MixtureOfExperts(dim, num_experts, expert_size, active_experts)
        
        # Layer 4: RMSNorm
        self.norm2 = RMSNorm(dim)
        
        # Layer 5: Grouped Query Attention (GQA)
        self.gqa_attention = GroupedQueryAttention(dim, num_heads, group_size=4)
        
        # Layer 6: MoE FFN Layer (High-Capacity) - 256 experts, 42 active
        self.moe_ffn2 = MixtureOfExperts(dim, num_experts, expert_size, active_experts)
        
        # Layer 7: RMSNorm
        self.norm3 = RMSNorm(dim)
        
        # Layer 8: Multi-Token Prediction (MTP) Head
        self.mtp_head = MultiTokenPredictionHead(dim, prediction_horizon=3, vocab_size=vocab_size)
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        # Layer 1: RMSNorm
        residual1 = x
        x = self.norm1(x)
        
        # Layer 2: Gated DeltaNet
        x = self.deltanet(x) + residual1
        
        # Layer 3: MoE FFN
        residual2 = x
        x = self.moe_ffn1(x) + residual2
        
        # Layer 4: RMSNorm
        residual3 = x
        x = self.norm2(x)
        
        # Layer 5: GQA Attention
        x = self.gqa_attention(x, mask) + residual3
        
        # Layer 6: MoE FFN
        residual4 = x
        x = self.moe_ffn2(x) + residual4
        
        # Layer 7: RMSNorm
        x = self.norm3(x)
        
        # Layer 8: MTP Head (returns predictions but keeps x for next block)
        predictions = self.mtp_head(x)
        
        return x, predictions


class WorldClassNeuralNetwork(nn.Module):
    """
    The complete world-class neural network with the architecture you described:
    - 2 initial layers: Embedding and Positional Encoding
    - 12 identical superblocks (each with 8 layers = 96 total block layers)
    - 3 final layers: Final RMSNorm, Output Layer (Unembedding), and Softmax/Sampling
    Total: ~99-101 layers with approximately 235 billion parameters
    """
    def __init__(self, 
                 vocab_size: int = 50432,  # Common vocab size for large models
                 dim: int = 8192,  # Hidden dimension
                 max_seq_len: int = 2048,  # Maximum sequence length
                 num_blocks: int = 12,  # Number of superblocks
                 num_heads: int = 16,
                 head_dim: int = 64,
                 num_experts: int = 256,  # 256 experts as per requirements
                 expert_size: int = 2048,
                 active_experts: int = 42):  # 42 active experts per token
        super().__init__()
        
        self.vocab_size = vocab_size
        self.dim = dim
        self.max_seq_len = max_seq_len
        self.num_blocks = num_blocks
        self.num_experts = num_experts
        self.active_experts = active_experts
        
        # Initial layers (2 total)
        # Embedding Layer
        self.embedding = nn.Embedding(vocab_size, dim)
        
        # Positional Encoding
        self.pos_encoding = self._create_positional_encoding(max_seq_len, dim)
        
        # Dropout for regularization
        self.dropout = nn.Dropout(0.1)
        
        # Stack of 12 identical superblocks (96 layers total)
        self.blocks = nn.ModuleList([
            SuperBlock(dim, num_heads, head_dim, num_experts, expert_size, active_experts, vocab_size) 
            for _ in range(num_blocks)
        ])
        
        # Final layers (3 total)
        # Final RMSNorm
        self.final_norm = RMSNorm(dim)
        
        # Output Layer (Unembedding)
        self.unembedding = nn.Linear(dim, vocab_size, bias=False)
        
        # Initialize weights
        self.apply(self._init_weights)
        
    def _create_positional_encoding(self, max_len: int, d_model: int) -> torch.Tensor:
        """Create sinusoidal positional encodings"""
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                            -(math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        return pe
    
    def _init_weights(self, module):
        """Initialize weights for the model"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)
    
    def forward(self, 
                input_ids: torch.Tensor, 
                attention_mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, List[List[torch.Tensor]]]:
        """
        Forward pass of the neural network
        """
        B, T = input_ids.shape
        
        # Initial processing: Embedding + Positional Encoding (2 layers)
        x = self.embedding(input_ids)  # [B, T, dim]
        
        # Add positional encoding
        pos_enc = self.pos_encoding[:T, :].to(x.device)
        x = x + pos_enc
        
        x = self.dropout(x)
        
        # Process through all superblocks (12 blocks Ã— 8 layers each = 96 layers)
        all_predictions = []
        for block in self.blocks:
            x, predictions = block(x, attention_mask)
            all_predictions.append(predictions)
        
        # Final processing (3 layers)
        # Final RMSNorm
        x = self.final_norm(x)
        
        # Output Layer (Unembedding)
        logits = self.unembedding(x)  # [B, T, vocab_size]
        
        # Softmax is applied during training/inference as needed
        return logits, all_predictions


def count_parameters(model: nn.Module) -> int:
    """Count the total number of trainable parameters in the model"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


# Example usage and testing
if __name__ == "__main__":
    # Create the model with approximate parameters for a large-scale model
    model = WorldClassNeuralNetwork(
        vocab_size=50432,
        dim=8192,  # Large hidden dimension
        max_seq_len=2048,
        num_blocks=12,  # As per your specification
        num_heads=16,
        head_dim=64,
        num_experts=256,  # 256 experts as per requirements
        expert_size=2048,
        active_experts=42  # 42 active experts per token
    )
    
    print(f"Model created successfully!")
    print(f"Total parameters: {count_parameters(model):,}")
    print(f"Target: ~235 billion parameters")
    
    # Test forward pass
    batch_size = 2
    seq_len = 128
    input_ids = torch.randint(0, 50432, (batch_size, seq_len))
    
    with torch.no_grad():
        logits, predictions = model(input_ids)
        
    print(f"Input shape: {input_ids.shape}")
    print(f"Output logits shape: {logits.shape}")
    print(f"Number of prediction sets: {len(predictions)}")  # Should be 12 blocks
    print(f"Predictions per block: {len(predictions[0])}")  # Should be 3 (prediction horizon)
    print("Forward pass completed successfully!")